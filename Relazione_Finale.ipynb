{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relazione finale\n",
    "\n",
    "**Gruppo:** Bug Data\n",
    "\n",
    "**Componenti del gruppo:** Jerin George Mathew, Luca Pasquini\n",
    "\n",
    "## 1. Indice\n",
    "La relazione è articolata nella seguente maniera:\n",
    "\n",
    "-  **Analisi del dataset**\n",
    "- **Specifiche hardware e software**\n",
    "- **Primo job**\n",
    "    - *Map reduce*\n",
    "    - *Hive*\n",
    "    - *Spark*\n",
    "    - *Risultati*\n",
    "    - *Grafici*\n",
    "- **Secondo job**\n",
    "    - *Map reduce*\n",
    "    - *Hive*\n",
    "    - *Spark*\n",
    "    - *Risultati*\n",
    "    - *Grafici*\n",
    "- **Terzo job**\n",
    "    - *Map reduce*\n",
    "    - *Hive*\n",
    "    - *Spark*\n",
    "    - *Risultati*\n",
    "    - *Grafici*\n",
    "- **Conclusioni**\n",
    "\n",
    "Verrà dunque dapprima analizzato e descritto il dataset a disposizione per poi discutere l'implementazione dei job richiesti dal progetto nelle varie tecnologie richieste dalle specifiche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analisi del dataset\n",
    "Verranno analizzati in questa sezione i due dataset a disposizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "HISTORICAL_STOCK_PRICES_FILEPATH = 'dataset/historical_stock_prices.csv'\n",
    "HISTORICAL_STOCKS_FILEPATH = 'dataset/historical_stocks.csv'\n",
    "\n",
    "hsp = pd.read_csv(HISTORICAL_STOCK_PRICES_FILEPATH)\n",
    "hs = pd.read_csv(HISTORICAL_STOCKS_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `historical_stock_prices.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20973889, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque il file `historical_stock_prices.csv` è composto da 209738889 righe e 8 colonne. Stampiamo ora un sottoinsieme delle righe del file `historical_stock_prices.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.58</td>\n",
       "      <td>8.493155</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.68</td>\n",
       "      <td>4633900</td>\n",
       "      <td>2013-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.66</td>\n",
       "      <td>11.55</td>\n",
       "      <td>8.471151</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.66</td>\n",
       "      <td>275800</td>\n",
       "      <td>2013-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.55</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8.507822</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.60</td>\n",
       "      <td>277100</td>\n",
       "      <td>2013-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.65</td>\n",
       "      <td>8.544494</td>\n",
       "      <td>11.55</td>\n",
       "      <td>11.65</td>\n",
       "      <td>147400</td>\n",
       "      <td>2013-05-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.60</td>\n",
       "      <td>11.53</td>\n",
       "      <td>8.456484</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.60</td>\n",
       "      <td>184100</td>\n",
       "      <td>2013-05-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker   open  close  adj_close    low   high   volume        date\n",
       "0    AHH  11.50  11.58   8.493155  11.25  11.68  4633900  2013-05-08\n",
       "1    AHH  11.66  11.55   8.471151  11.50  11.66   275800  2013-05-09\n",
       "2    AHH  11.55  11.60   8.507822  11.50  11.60   277100  2013-05-10\n",
       "3    AHH  11.63  11.65   8.544494  11.55  11.65   147400  2013-05-13\n",
       "4    AHH  11.60  11.53   8.456484  11.50  11.60   184100  2013-05-14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come già detto nelle specifiche, il file `historical_stock_prices.csv` è composto dai seguenti campi:\n",
    "- ticker: simbolo dell’azione\n",
    "- open: prezzo di apertura\n",
    "- close: prezzo di chiusura\n",
    "- adj_close: prezzo di chiusura “modificato”\n",
    "- lowThe: prezzo minimo\n",
    "- highThe: prezzo massimo\n",
    "- volume: numero di transazioni\n",
    "- date: data nel formato aaaa-mm-gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una attività preliminare all'implementazione vera e propria dei job è quella di data cleaning in cui si va a verificare la presenza di eventuali record non corretti (ad esempio contenenti valori nulli) che potrebbero essere presenti nel dataset. Verifichiamo in particolare che non siano presenti valori nulli nel file `historical_stock_prices.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsp.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque non sono presenti valori nulli. Una altra attività che possiamo effettuare è andare a verificare che vi sia un record per ogni coppia `(ticker, data)`, ovvero che non siano presenti record duplicati per un certo `ticker` in una certa data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hsp.groupby(['ticker', 'date'])) != hsp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque non sono presenti record duplicati. \n",
    "\n",
    "Passiamo ora ad analizzare il secondo dataset, `historical_stocks.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 `historical_stocks.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6460, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>exchange</th>\n",
       "      <th>name</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIHPP</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TURN</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>180 DEGREE CAPITAL CORP.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>FINANCE/INVESTORS SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLWS</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1-800 FLOWERS.COM, INC.</td>\n",
       "      <td>CONSUMER SERVICES</td>\n",
       "      <td>OTHER SPECIALTY STORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FCCY</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1ST CONSTITUTION BANCORP (NJ)</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>SAVINGS INSTITUTIONS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker exchange                                    name             sector  \\\n",
       "0    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.            FINANCE   \n",
       "1  PIHPP   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.            FINANCE   \n",
       "2   TURN   NASDAQ                180 DEGREE CAPITAL CORP.            FINANCE   \n",
       "3   FLWS   NASDAQ                 1-800 FLOWERS.COM, INC.  CONSUMER SERVICES   \n",
       "4   FCCY   NASDAQ           1ST CONSTITUTION BANCORP (NJ)            FINANCE   \n",
       "\n",
       "                     industry  \n",
       "0  PROPERTY-CASUALTY INSURERS  \n",
       "1  PROPERTY-CASUALTY INSURERS  \n",
       "2  FINANCE/INVESTORS SERVICES  \n",
       "3      OTHER SPECIALTY STORES  \n",
       "4        SAVINGS INSTITUTIONS  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque il secondo dataset contiene 6460 righe e 5 campi:\n",
    "- ticker: simbolo dell’azione\n",
    "- exchange: NYSE o NASDAQ\n",
    "- name: nome dell’azienda\n",
    "- sector: settore dell’azienda\n",
    "- industry: industria di riferimento per l’azienda\n",
    "\n",
    "Vediamo anche per questo secondo dataset se sono presenti valori nulli e/o duplicati e stampiamo alcune statistiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker      False\n",
       "exchange    False\n",
       "name        False\n",
       "sector       True\n",
       "industry     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo dunque come siano presenti dei campi, in particolare `sector` e `industry`  nel secondo dataset che presentano valori nulli. Stampiamo ora qualche record contenente valori nulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>exchange</th>\n",
       "      <th>name</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ABP</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>ABPRO CORPORATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SQZZ</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>ACTIVE ALTS CONTRARIAN ETF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>ACT</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>ADVISORSHARES VICE ETF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ABDC</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>ALCENTRA CAPITAL CORP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>SMCP</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>ALPHAMARK ACTIVELY MANAGED SMALL CAP ETF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker exchange                                      name sector industry\n",
       "19     ABP   NASDAQ                         ABPRO CORPORATION    NaN      NaN\n",
       "42    SQZZ   NASDAQ                ACTIVE ALTS CONTRARIAN ETF    NaN      NaN\n",
       "62     ACT   NASDAQ                    ADVISORSHARES VICE ETF    NaN      NaN\n",
       "100   ABDC   NASDAQ                    ALCENTRA CAPITAL CORP.    NaN      NaN\n",
       "124   SMCP   NASDAQ  ALPHAMARK ACTIVELY MANAGED SMALL CAP ETF    NaN      NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[hs['sector'].isnull() | hs['industry'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fase di implementazione dei job occorrerà dunque tenere a mente di ignorare i record contenenti valori nulli di `sector` e `industry`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hs['ticker']) != hs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dunque non sono presenti valori duplicati per il file `historical_stocks.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 `historical_stocks.csv` + `historical_stock_prices.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo ora il join tra i due dataset per effettuare poi ulteriori attività di analisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(hs, hsp, on = 'ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20973889, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>exchange</th>\n",
       "      <th>name</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.95</td>\n",
       "      <td>7.95</td>\n",
       "      <td>7.90</td>\n",
       "      <td>8.50</td>\n",
       "      <td>642900</td>\n",
       "      <td>2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "      <td>7.94</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.16</td>\n",
       "      <td>7.90</td>\n",
       "      <td>8.29</td>\n",
       "      <td>228400</td>\n",
       "      <td>2014-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "      <td>8.29</td>\n",
       "      <td>8.39</td>\n",
       "      <td>8.39</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.40</td>\n",
       "      <td>105000</td>\n",
       "      <td>2014-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.69</td>\n",
       "      <td>8.32</td>\n",
       "      <td>8.70</td>\n",
       "      <td>113600</td>\n",
       "      <td>2014-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.94</td>\n",
       "      <td>8.94</td>\n",
       "      <td>8.55</td>\n",
       "      <td>9.00</td>\n",
       "      <td>60500</td>\n",
       "      <td>2014-04-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker exchange                                    name   sector  \\\n",
       "0    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.  FINANCE   \n",
       "1    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.  FINANCE   \n",
       "2    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.  FINANCE   \n",
       "3    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.  FINANCE   \n",
       "4    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.  FINANCE   \n",
       "\n",
       "                     industry  open  close  adj_close   low  high  volume  \\\n",
       "0  PROPERTY-CASUALTY INSURERS  8.00   7.95       7.95  7.90  8.50  642900   \n",
       "1  PROPERTY-CASUALTY INSURERS  7.94   8.16       8.16  7.90  8.29  228400   \n",
       "2  PROPERTY-CASUALTY INSURERS  8.29   8.39       8.39  8.05  8.40  105000   \n",
       "3  PROPERTY-CASUALTY INSURERS  8.50   8.69       8.69  8.32  8.70  113600   \n",
       "4  PROPERTY-CASUALTY INSURERS  9.00   8.94       8.94  8.55  9.00   60500   \n",
       "\n",
       "         date  \n",
       "0  2014-04-01  \n",
       "1  2014-04-02  \n",
       "2  2014-04-03  \n",
       "3  2014-04-04  \n",
       "4  2014-04-07  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad esempio possiamo verificare se una certa compagnia dispone di più di una azione (ovvero la presenza di un un nome (`name`) di compagnia associato a più di un `ticker`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('name')['ticker'].nunique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BARCLAYS PLC'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('name')['ticker'].nunique().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BARCLAYS PLC', 'BCS'),\n",
       " ('BARCLAYS PLC', 'DFVL'),\n",
       " ('BARCLAYS PLC', 'DFVS'),\n",
       " ('BARCLAYS PLC', 'DLBL'),\n",
       " ('BARCLAYS PLC', 'DLBS'),\n",
       " ('BARCLAYS PLC', 'DTUL'),\n",
       " ('BARCLAYS PLC', 'DTUS'),\n",
       " ('BARCLAYS PLC', 'DTYL'),\n",
       " ('BARCLAYS PLC', 'DTYS'),\n",
       " ('BARCLAYS PLC', 'FLAT'),\n",
       " ('BARCLAYS PLC', 'STPP'),\n",
       " ('BARCLAYS PLC', 'TAPR')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['name'] == 'BARCLAYS PLC'].groupby(['name', 'ticker']).groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo notare ad esempio come **_BARCLAYS PLC_** abbia ben 12 `ticker`.\n",
    "\n",
    "Verifichiamo ora se vi sono aziende (nomi di aziende) associate a più di un settore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('name')['sector'].nunique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENERGIZER HOLDINGS, INC.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('name')['sector'].nunique().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENERGIZER HOLDINGS, INC.', 'CONSUMER NON-DURABLES'),\n",
       " ('ENERGIZER HOLDINGS, INC.', 'MISCELLANEOUS')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['name'] == 'ENERGIZER HOLDINGS, INC.'].groupby(['name', 'sector']).groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo notare ad esempio come **_ENERGIZER HOLDINGS, INC._** sia associato a due settori.\n",
    "\n",
    "Un'ulteriore controllo che possiamo effettuare sta nel verificare se sono presenti compagnie che sono quotate sia nel NYSE che nel NASDAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('name')['exchange'].nunique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMTRUST FINANCIAL SERVICES, INC.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('name')['exchange'].nunique().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AMTRUST FINANCIAL SERVICES, INC.', 'NASDAQ'),\n",
       " ('AMTRUST FINANCIAL SERVICES, INC.', 'NYSE')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['name'] == 'AMTRUST FINANCIAL SERVICES, INC.'].groupby(['name', 'exchange']).groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo come ad esempio **_AMTRUST FINANCIAL SERVICES, INC._** sia quotata in entrambe le borse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Specifiche hardware e software\n",
    "\n",
    "Verranno ora fornite le specifiche hardware e software della macchina locale e del cluster su cui sono stati eseguiti i job.\n",
    "\n",
    "Si precisa come job MapReduce siano stati scritti in Python ed eseguiti utilizzando _Hadoop Streaming_(https://hadoop.apache.org/docs/r3.0.3/hadoop-streaming/HadoopStreaming.html). Hadoop Streaming è in particolare è una utility che consente agli sviluppatori di poter scrivere il codice relativo al mapper e al reducer in qualsiasi linguaggio (oltre a Java), come ad esempio Python o Scala.\n",
    "\n",
    "Per quanto riguarda Spark, gli script sono stati scritti sempre in Python.\n",
    "\n",
    "### 3.1 Macchina locale\n",
    "\n",
    "#### 3.1.1 Specifiche hardware\n",
    "\n",
    "- Intel i5 (5257U) Dual core a 2.7GHz\n",
    "- 8 GB di RAM\n",
    "- 128 GB di memoria secondaria (SSD)\n",
    "\n",
    "#### 3.1.2 Specifiche software\n",
    "\n",
    "- Java 1.8.0_181\n",
    "- Python 3.7.3\n",
    "- Hadoop 3.1.2\n",
    "- Hive 2.3.4\n",
    "- Spark 2.4.3\n",
    "\n",
    "### 3.2 Cluster\n",
    "Il cluster è stato realizzato utilizzando il servizio Dataproc di Google Cloud Platform. In particolare il cluster è composto da un nodo master e 3 nodi worker. Sia i nodi master che i nodi worker hanno le stesse specifiche hardware e software. I dataset sono stati invece salvati all'interno di un bucket di Google Cloud Storage (servizio cloud di storage di Google Cloud Platform).\n",
    "\n",
    "#### 3.2.1 Specifiche hardware\n",
    "\n",
    "- Macchine virtuali di tipo n1-standard-4 (4 vCPU, 15 GB di RAM)\n",
    "- 500 GB di spazio su disco (hard disk)\n",
    "\n",
    "#### 3.2.2 Specifiche software\n",
    "\n",
    "- Python 3.6\n",
    "- Hadoop 2.9.2\n",
    "- Hive 2.3.4\n",
    "- Spark 2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Primo Job\n",
    "\n",
    "### 4.1 Specifiche\n",
    "Implementare un job che sia in grado di generare, in ordine, le dieci azioni la cui quotazione (prezzo di chiusura) è cresciuta maggiormente dal 1998 al 2018, indicando, per ogni azione: (a) il simbolo, (b) l’incremento percentuale, (c) il prezzo minimo raggiunto, (e) quello massimo e (f) il volume medio giornaliero in quell’intervallo temporale.\n",
    "\n",
    "### 4.2 Implementazione in MapReduce\n",
    "\n",
    "**Mapper**\n",
    "\n",
    "In fase di mapping estrapoliamo innanzitutto da ciascun record i campi `ticker`, `close`, `low`, `high`, `volume` e `date`.\n",
    "\n",
    "Successivamente verifichiamo che il record sia relativo ad un anno che ricada nell'intervallo 1998-2018, per poi stampare la chiave i valori associati alla chiave. \n",
    "In particolare la chiave è composta da due campi: `ticker`, usata come chiave \"primaria\", e `date` usata come chiave \"secondaria\".\n",
    "\n",
    "In questo modo valori che verranno ricevuti dal reducer saranno aggregati per il solo campo `ticker` e ordinati in base al campo `date` (si parla in questo caso di _secondary sort_). Questo consentirà nella fase di reduce di evitare comparazioni tra date (per trovare la data più recente e quella meno recente) ai fini del calcolo della differenza percentuale.\n",
    "\n",
    "\n",
    "Il valore associato alla chiave è invece composto dai campi `close`, `low`, `high` e `volume`.\n",
    "\n",
    "```python\n",
    "class mapper:\n",
    "    \n",
    "    map(key, record):\n",
    "        ticker, _, close, _, low, high, volume, date = record\n",
    "        year = getYear(date)\n",
    "        if year in range from 1998 to 2018:\n",
    "            key = ticker, date\n",
    "            value = close, low, high, volume\n",
    "            Emit(key, value)\n",
    "```\n",
    "\n",
    "**Reducer**\n",
    "\n",
    "Durante la fase di reduce definiamo una variabile globale `result` contenente una lista di strutture dati che chiameremo di seguito `item`, ciascuno dei quali contiene i seguenti campi:\n",
    "\n",
    "- ticker\n",
    "- differenza percentuale\n",
    "- volume medio\n",
    "- prezzo minimo\n",
    "- prezzo massimo\n",
    "\n",
    "In particolare ciascun `item` viene generato a partire da un data coppia (ticker,valori) in input alla fase di reduce.\n",
    "\n",
    "Si descrive ora come vengono calcolati gli ultimi 4 campi di questa struttura dati.\n",
    "\n",
    "In particolare, per calcolare la differenza percentuale estraiamo dalla lista `records` (si veda lo pseudocodice) il campo `close` del primo e dell'ultimo elemento della lista, essendo i valori associati ai ticker (ovvero la chiave) ordinati per data. Si procede poi al calcolo della differenza percentuale. \n",
    "\n",
    "Per quanto riguarda il volume medio, si estraggono i valori di `volume` associati al `ticker` corrente, si sommano tali valori e si divide il risultato per il numero di `volume`. \n",
    "\n",
    "Infine per poter calcolare il prezzo massimo e minimo, si estraggono i campi `low` e `high` da ciascun elemento della lista `records` per individuare poi il prezzo minimo e massimo. \n",
    "\n",
    "I valori così calcolati vengono poi salvati nella struttura dati `item`. A questo punto si verifica che la data meno recente del ticker corrente sia il 1998 come pure che la data più recente associata al `ticker` processato sia nell'anno 2018 (ai fini di filtrare le aziende che esistono tutt'ora oggi) e, in caso di esito positivo, la struttura dati `item` così computata viene aggiunta alla lista `result`. \n",
    "\n",
    "Infine, una volta computati tutti i ticker, la lista `result` viene ordinata in base al campo `percentChange` dei suoi elementi (in ordine decrescente) e vengono stampati i primi 10 di tale lista ordinata.\n",
    "\n",
    "```python\n",
    "class reducer:\n",
    "    \n",
    "    setup():\n",
    "        result = empty list\n",
    "\n",
    "    reduce(ticker, records):\n",
    "        totalVolume = 0\n",
    "        count = 0\n",
    "        minLow = infinity\n",
    "        maxHigh = - infinity\n",
    "        \n",
    "        \n",
    "        # get percent change\n",
    "        startingClosePrice = values.getFirstElement().getClose()\n",
    "        endingClosePrice = values.getLastElement.getClose()\n",
    "        percentChange = (endingClosePrice - startingClosePrice)/startingClosePrice\n",
    "        \n",
    "        # compute remaining values \n",
    "        for each record in records:\n",
    "            totalVolume += volume value for current record\n",
    "            count += 1\n",
    "            minLow = min(minLow, low price value for current record)\n",
    "            maxHigh = max(maxHigh, high price value for current record)\n",
    "        \n",
    "        # get volume\n",
    "        averageVolume = totalVolume/count\n",
    "\n",
    "        # add this item to result list\n",
    "        startingDate = values.getFirstElement().getYear()\n",
    "        endingDate = values.getLastElement().getYear()\n",
    "        if startingDate == 1998 and endingDate == 2018:\n",
    "            obj = {ticker, percentChange, minLow, maxHigh, averageVolume}\n",
    "            result.append(obj)\n",
    "    \n",
    "    cleanup()\n",
    "        sortedResult = sortByPercentChange(results, reverse=True)\n",
    "        for i in range(10):\n",
    "            Emit(sortedResult.getItem(i))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementazione in Hive\n",
    "\n",
    "Vengono definite complessivamente 6 viste:\n",
    "\n",
    "- `ticker_min_max_avg`, contenente il minimo valore di `low` e il massimo valore di `high` per ogni `ticker`\n",
    "- `ticker_min_data` e `ticker_close_min_data`, contenenti la data meno recente e il corrispondente valore di `close` per ciascun `ticker` rispettivamente\n",
    "- `ticker_max_data` e `ticker_close_max_data`, contenenti la data più recente e il corrispondente valore di `close` per ciascun `ticker` rispettivamente\n",
    "- `ticker_percentuale`, che calcola l'incremento percentuale per ciascun `ticker` a partire dalle viste precedenti\n",
    "\n",
    "Infine viene effettuato un join tra le viste `ticker_min_max_avg` e `ticker_percentuale` per ottenere il risultato richiesto.\n",
    "\n",
    "```SQL\n",
    "CREATE VIEW IF NOT EXISTS ticker_min_max_avg AS \n",
    "SELECT ticker, min(low) AS min_price, max(high) AS max_price, avg(volume) AS avg_volume \n",
    "FROM historical_stock_prices \n",
    "WHERE YEAR(data)>=1998 AND YEAR(data)<=2018 \n",
    "GROUP BY ticker;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS ticker_min_data AS \n",
    "SELECT ticker, min(TO_DATE(data)) AS min_data \n",
    "FROM historical_stock_prices \n",
    "WHERE YEAR(data)==1998 \n",
    "GROUP BY ticker;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS ticker_max_data AS \n",
    "SELECT ticker, max(TO_DATE(data)) AS max_data \n",
    "FROM historical_stock_prices \n",
    "WHERE YEAR(data)==2018 \n",
    "GROUP BY ticker;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS ticker_close_min_data AS \n",
    "SELECT h.ticker, h.data, h.close \n",
    "FROM ticker_min_data AS t, historical_stock_prices AS h \n",
    "WHERE h.ticker=t.ticker AND h.data=t.min_data;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS ticker_close_max_data AS \n",
    "SELECT h.ticker, h.data, h.close \n",
    "FROM ticker_max_data AS t, historical_stock_prices AS h \n",
    "WHERE h.ticker=t.ticker AND h.data=t.max_data;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS ticker_percentuale AS \n",
    "SELECT mi.ticker, ((ma.close-mi.close)/mi.close) AS inc_perc \n",
    "FROM ticker_close_max_data AS ma JOIN ticker_close_min_data AS mi ON ma.ticker=mi.ticker;\n",
    "\n",
    "INSERT OVERWRITE LOCAL DIRECTORY 'output/'\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t' \n",
    "SELECT a.ticker, b.inc_perc, a.min_price, a.max_price, a.avg_volume \n",
    "FROM ticker_min_max_avg AS a join ticker_percentuale AS b on a.ticker=b.ticker \n",
    "ORDER BY b.inc_perc DESC limit 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Implementazione in Spark\n",
    "\n",
    "Si descrive ora l'implementazione del job in Spark. Vengono principalmente definiti i seguenti `RDD`:\n",
    "\n",
    "- `min_ticker_low`, contenente per ciasun ticker il prezzo di chiusura minimo\n",
    "- `max_ticker_high`, contenente per ciasun ticker il prezzo di chiusura massimo\n",
    "- `avg_ticker_volume`, contenente per ciascun ticker il volume medio giornaliero\n",
    "- `min_data_close`, che associa ad un dato ticker il prezzo di chiusura relativo alla data meno recente\n",
    "- `max_data_close`, che associa ad un dato ticker il prezzo di chiusura relativo alla data più recente\n",
    "\n",
    "Si effettua poi il join tra `min_data_close` e `max_data_close` per ottenere `join_inc_perc`. \n",
    "\n",
    "A partire da `join_inc_perc` viene calcolato l'incremento percentuale per ciascun ticker (`inc_perc`). \n",
    "\n",
    "Infine viene fatto il join tra `inc_perc`,`avg_ticker_volume`, `min_ticker_low` e `max_ticker_high`, per poi ordinare il risultato in base all'incremento percentuale (in ordine decrescente) estraendo infine i primi 10 elementi di tale `RDD` ordinato.\n",
    "\n",
    "```python\n",
    "input = leggi tutte le righe del file historical_stock_prices.csv\n",
    "        con data compresa tra il 1998 e il 2018\n",
    "    \n",
    "persist input in memory (and optionally on disk in case the RDD does not fit in memory) \n",
    "\n",
    "min_ticker_low = input.map(riga -> (ticker, low))\n",
    "\t\t              .reduceByKey(min(low1, low2))\n",
    "\n",
    "max_ticker_high = input.map(riga -> (ticker, high))\n",
    "\t\t\t           .reduceByKey(max(high1, high2))\n",
    "\n",
    "avg_ticker_volume = input.map(riga -> (ticker, (volume,1)))\n",
    "                         .reduceByKey((volume1+volume2, count+1))\n",
    "                         .map(riga -> (ticker, TotVolume/count))\n",
    "\n",
    "min_data_close = input.map(riga -> (ticker, (close, data)))\n",
    "                      .reduceByKey(minimo((data1,close1), (data2,close2)))\n",
    "                      .filter(data.year == \"1998\")\t\n",
    "\n",
    "max_data_close = input.map(riga -> (ticker, (close, data)))\n",
    "                      .reduceByKey(massimo((data1,close1), (data2,close2)))\n",
    "                      .filter(data.year == \"2018\")\n",
    "\n",
    "join_inc_perc = min_data_close.join(max_data_close)\n",
    "    \n",
    "inc_perc = join_inc_perc\n",
    "                .map(riga -> (ticker, (maxclose-minclose)/minclose))\n",
    "\n",
    "result = max_ticker_high\n",
    "            .join(min_ticker_low)\n",
    "            .join(inc_perc)\n",
    "            .join(avg_ticker_volume)\n",
    "            .sortBy(incremento percentuale decrescente)\n",
    "            .take(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Risultati\n",
    "\n",
    "Si mostra ora l'output restituito per il primo job:\n",
    "\n",
    "```\n",
    "Ticker Percent Change Min low          Max high\t   Average volume\n",
    "MNST  163340.387616%  0.0305979158729  70.2200012207  7347898.8208\n",
    "AMZN  38328.032677%   4.14583349228    1925.0         7868702.73287\n",
    "AAPL  37146.0319467%  0.482142865658   219.179992676  121398558.199\n",
    "CTSH  36312.8011611%  0.145833328366   85.0999984741  6272137.93307\n",
    "CELG  24924.0000849%  0.171875         147.169998169  8002695.57352\n",
    "WP    24012.4988778%  0.0500000007451  96.5100021362  1270066.16934\n",
    "MED   13733.2303561%  0.0936999991536  229.199996948  223768.309139\n",
    "NVR   11786.7001488%  21.625           3700.0         56463.7413395\n",
    "ANSS  10077.1432059%  1.375            184.949996948  482841.405197\n",
    "TSCO  9508.67816472%  0.40625          97.25          1592298.53705\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Grafici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Secondo Job\n",
    "\n",
    "### 5.1 Specifiche\n",
    "Realizzare un job che sia in grado di generare, per ciascun settore, il relativo “trend” nel periodo 2004-2018 ovvero un elenco contenente, per ciascun anno nell’intervallo: (a) il volume complessivo del settore, (b) la percentuale di variazione annuale (differenza percentuale arrotondata tra la quotazione di fine anno e quella di inizio anno) e (c) la quotazione giornaliera media. N.B.: volume e quotazione di un settore si ottengono sommando i relativi valori di tutte le azioni del settore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Implementazione in MapReduce\n",
    "\n",
    "**Mapper**\n",
    "\n",
    "Leggiamo dapprima, utilizzando la _Distributed Cache_ (https://hadoop.apache.org/docs/r3.0.0/api/org/apache/hadoop/filecache/DistributedCache.html) il file `historical_stocks.csv`, per poi definire, a partire dallo stesso file, una struttura dati (`tickerToSectorMap`) che associa a ciascun `ticker` il corrispondente settore (escludendo i `ticker` privi di un corrispondente settore).\n",
    "\n",
    "La _Distributed Cache_ in particolare consente poter mettere a disposizione dei nodi worker, ovvero mapper e reducer, dei file che possono poi essere letti ai fini di svolgere le attività di map e reduce rispettivamente. \n",
    "\n",
    "La struttura dati `tickerToSectorMap` così creata verrà usata poi in fase di mapping per poter effettuare il \"join\" con i record del file `historical_stock_prices.csv` associando a ciascun record il settore del `ticker` presente nel record stesso.\n",
    "\n",
    "In fase di mapping estrapoliamo da ciascun record i campi `ticker`, `close`, `volume` e `date`, verifichiamo che il record sia relativo ad un anno che ricada nell'intervallo 2004-2018 per poi stampare la coppia chiave valore associata al record in questione.\n",
    "\n",
    "In particolare la chiave è composta da tre campi: `sector`, usata come chiave \"primaria\", `ticker` e `date`, dove questi ultimi due campi vengono usati per poter effettuare il _secondary sort_, in maniera tale che i valori per uno stesso settore siano ordinati (e aggregati) per `sector` e a parità di `sector` siano ordinati per `ticker` e `data`.\n",
    "\n",
    "Grazie a questo ordinamento delle chiavi sarà poi possibile in fase di reduce poter calcolare gli incrementi percentuali associati a ciascun settore senza dover effettuare una comparazione tra date (al fine di trovare la data più recente e quella meno recente).\n",
    "\n",
    "Il valore associato alla chiave è invece composta dai campi `close` e `volume`.\n",
    "\n",
    "```python\n",
    "class mapper:\n",
    "    \n",
    "    setup():\n",
    "        read the historical_stocks.csv file from the Distributed Cache\n",
    "        tickerToSectorMap = map which associate a (non null) sector for each ticker in historical_stocks.csv\n",
    "    \n",
    "    map(key, record):\n",
    "        ticker, _, close, _, _, _, volume, date = record\n",
    "        year = getYear(date)\n",
    "        if year in range from 2004 to 2018 and \n",
    "           ticker has a corresponding sector in tickerToSectorMap:\n",
    "            sector = ticker's corresponding sector\n",
    "            key = sector, ticker, date\n",
    "            value = close, volume\n",
    "            Emit(key, value)\n",
    "```\n",
    "\n",
    "**Reducer**\n",
    "\n",
    "Durante la fase di reduce, per ciascun settore definiamo una serie di strutture dati (nello specifico mappe chiave valore) finalizzate al calcolo del volume complessivo, della quotazione giornaliera media e dell'incremento percentuale per settore per ciascun anno:\n",
    "\n",
    "Verrà ora descritto come vengono calcolati questi tre valori.\n",
    "\n",
    "Per calcolare la differenza percentuale si sfrutta l'ordinamento dei valori associati a ciascun settore per poter estrarre i prezzi di chiusura meno recenti e più recenti di ciascun `ticker` in ciascun anno. Si sommano poi tali valori per ciascun anno per ottenere la \"quotazione di partenza\" annua e la \"quotazione finale\" annua del settore. A partire da questi ultimi due valori si calcola infine la differenza percentuale. \n",
    "\n",
    "Per il calcolo della quotazione giornaliera media in un dato anno invece si sommano dapprima i prezzi di chiusura dei ticker relativi stessa data. Si effettua poi una media aritmetica dei risultati così ottenuti.\n",
    "\n",
    "Infine per quanto riguarda il volume complessivo annuo, si scandiscono i record del settore in questione, sommando tra loro i valori di `volume` relativi allo stesso anno.\n",
    "\n",
    "```python\n",
    "class reducer:\n",
    "\n",
    "    reduce(sector, records):\n",
    "        yearToStartingCloseValueMap = empty map\n",
    "        yearToEndingCloseValueMap = empty map\n",
    "        yearToTotalVolumeMap = empty map\n",
    "        yearToTotalCloseValue = empty nested map\n",
    "        \n",
    "        for record in records:\n",
    "            day = get date field from the current record\n",
    "            year = get year value from the day variable\n",
    "            if the record's corresponding ticker has changed from the previous record:\n",
    "                yearToEndingCloseValueMap[previous year] += close field taken from the previous record\n",
    "                yearToStartingCloseValueMap[year] += close field taken from the current record\n",
    "            yearToTotalVolumeMap[year] += volume field taken from the current record\n",
    "            yearToTotalCloseValue[year][date] += close\n",
    "            \n",
    "        percentChange = empty map\n",
    "  \n",
    "        for each year from 2004 to 2018:\n",
    "            difference = yearToEndingCloseValueMap[year] - yearToStartingCloseValueMap[year]\n",
    "            percentChange[year] = difference/yearToStartingCloseValueMap[year]\n",
    "            count = number of date keys in yearToTotalCloseValue[year][date]\n",
    "            dailyCloseAverage[year] = yearToTotalCloseValue[year][date]/count\n",
    "            Emit(sector, totalVolume[year], percentChange[year], dailyCloseAverage[year])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Implementazione in Hive\n",
    "\n",
    "Viene dapprima effettuato un join tra le tabelle che contengono i file `historical_stock_prices.csv` e `historical_stocks.csv` mettendo la tabella che contiene i record giornalieri a destra dell'operatore di join per migliorare l'efficienza dell'operazione. Durante l'operazione di join vengono filtrati i ticker che non sono associati ad alcun settore.\n",
    "Vengono in seguito definite 7 viste:\n",
    "\n",
    "- `sector_data_volume`, contenente il volume complessivo annuo di ciascun settore;\n",
    "- `sector_data_min_max`, che contiene per ciascun ticker e ciascun anno la data meno recente e quella più recente;\n",
    "- `sector_data_close_min` e `sector_data_close_max`, che contengono il prezzo di chiusura complessivo associato alla data meno recente e quella più recente per ciascun settore in ciascun anno;\n",
    "- `sector_data_close`, che contiene l'incremento percentuale per ciascun settore in ciascun anno;\n",
    "- `sector_data_sum_close`, che contiene la somma dei prezzi di chiusura di uno stesso giorno per tutti i ticker di uno stesso settore\n",
    "- `sector_data_avg_close`, contenente la quotazione giornaliera media per ciascun settore in ciascun anno\n",
    "\n",
    "Viene infine effettuato il join tra le tabelle `sector_data_avg_close`, `sector_data_close`, `sector_data_volume` per ottenere l'output richiesto.\n",
    "\n",
    "\n",
    "```SQL\n",
    "CREATE TABLE IF NOT EXISTS first_pricipal_table AS \n",
    "SELECT hs.sector, hsp.ticker, hsp.data, hsp.close, hsp.volume \n",
    "FROM historical_stock AS hs JOIN historical_stock_prices AS hsp ON hsp.ticker=hs.ticker \n",
    "WHERE YEAR(hsp.data)>=2004 AND YEAR(hsp.data)<=2018 AND hs.sector!='N/A';\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS sector_data_volume AS \n",
    "SELECT sector, YEAR(data) AS anno, SUM(volume) AS somma_volume \n",
    "FROM first_pricipal_table \n",
    "GROUP BY sector, YEAR(data);\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS sector_data_min_max AS \n",
    "SELECT sector, ticker, min(TO_DATE(data)) AS min_data, max(TO_DATE(data)) AS max_data \n",
    "FROM first_pricipal_table \n",
    "GROUP BY sector, ticker, YEAR(data);\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS sector_data_close_min AS \n",
    "SELECT b.sector, YEAR(b.min_data) AS anno, SUM(a.close) AS min_close \n",
    "FROM first_pricipal_table AS a, sector_data_min_max AS b \n",
    "WHERE a.sector=b.sector AND a.data=b.min_data AND b.ticker=a.ticker \n",
    "GROUP BY b.sector, YEAR(b.min_data);\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS sector_data_close_max AS \n",
    "SELECT b.sector, YEAR(b.max_data) AS anno, SUM(a.close) AS max_close \n",
    "FROM first_pricipal_table AS a, sector_data_min_max AS b \n",
    "WHERE a.sector=b.sector AND a.data=b.max_data AND b.ticker=a.ticker \n",
    "GROUP BY b.sector, YEAR(b.max_data);\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS sector_data_close AS \n",
    "SELECT mi.sector, mi.anno, ROUND((ma.max_close-mi.min_close)/mi.min_close*100,2) AS perc_var_anno \n",
    "FROM sector_data_close_min AS mi, sector_data_close_max AS ma \n",
    "WHERE mi.sector=ma.sector AND mi.anno=ma.anno\n",
    "ORDER BY sector, anno; \n",
    "\n",
    "CREATE VIEW IF NOT EXISTS sector_data_sum_close AS \n",
    "SELECT sector, data, SUM(close) AS somma \n",
    "FROM first_pricipal_table \n",
    "GROUP BY sector, data;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS sector_data_avg_close AS \n",
    "SELECT sector, YEAR(data) AS anno, AVG(somma) AS media \n",
    "FROM sector_data_sum_close \n",
    "GROUP BY sector, YEAR(data);\n",
    "\n",
    "INSERT OVERWRITE LOCAL DIRECTORY 'output/'\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t' \n",
    "SELECT a.sector, a.anno, c.somma_volume, b.perc_var_anno, a.media \n",
    "FROM sector_data_avg_close AS a, sector_data_close AS b, sector_data_volume AS c \n",
    "WHERE a.sector=b.sector AND b.sector=c.sector AND a.anno=b.anno AND c.anno=b.anno\n",
    "ORDER BY sector, anno;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Implementazione in Spark\n",
    "\n",
    "\n",
    "Si descrive ora l'implementazione del job in Spark. Vengono principalmente definiti i seguenti `RDD`:\n",
    "\n",
    "- `sum_volume`, che contiene per ciascun settore, in ciascun anno, il volume complessivo;\n",
    "- `sum_close_avg`, che contiene per ciascun settore, in ciascun anno, la quotazione giornaliera media;\n",
    "- `min_data_close`, che contiene per ciascun settore, in ciascun anno, la quotazione relativa alla data meno recente;\n",
    "- `max_data_close`, che contiene per ciascun settore, in ciascun anno, la quotazione relativa alla data più recente;\n",
    "\n",
    "Si effettua poi il join tra `min_data_close` e `max_data_close` per ottenere `join_inc_perc`. \n",
    "\n",
    "A partire da `join_inc_perc` viene calcolato l'incremento percentuale per ciascun settore in ciascun anno (`inc_perc`). \n",
    "\n",
    "Infine viene fatto il join tra `inc_perc`,`sum_close_avg` e  `sum_volume` per ottenere l'output richiesto.\n",
    "\n",
    "```python\n",
    "hsp = RDD contenente il dataset historical_stock_prices.csv\n",
    "      con i record relativi agli anni nell'intervallo 2004-2018\n",
    "hs = RDD contenente il dataset historical_stocks.csv,\n",
    "     filtrando i record in cui il campo sector assume valore \"N/A\"\n",
    "    \n",
    "join_hsp_hs = join tra gli RDD hs e hsp\n",
    "\n",
    "persist join_hsp_hs in memory (and optionally on disk in case the RDD does not fit in memory) \n",
    "\n",
    "sum_volume = join_hsp_hs.map(riga -> ((sector, anno), volume)) \\\n",
    "                        .reduceByKey(volume1+volume2)\n",
    "\n",
    "sum_close_avg = join_hsp_hs.map(lambda line: ((sector, date), close)) \\\n",
    "                           .reduceByKey(close1+close2) \\\n",
    "                           .map(lambda line: ((sector, anno), (somma_close, 1))) \\\n",
    "                           .reduceByKey(riga -> (somma_close1+somma_close2, count+1))\n",
    "                           .map(riga -> (sector, (somma_close/count))\n",
    "\n",
    "min_data_close = join_hsp_hs.map(riga -> ((ticker, sector, anno), (close, data)))\n",
    "                            .reduceByKey(minimo((data1,close1), (data2,close2)))\n",
    "                            .map(riga -> ((settore, anno), close_minimo)) \\\n",
    "                            .reduceByKey(close_minimo1+close_minimo2)\n",
    "\n",
    "max_data_close = join_hsp_hs.map(riga -> ((ticker, sector, anno), (close, data)))\n",
    "                            .reduceByKey(massimo((data1,close1), (data2,close2)))\n",
    "                            .map(riga -> ((settore, anno), close_massimo)) \\\n",
    "                            .reduceByKey(close_massimo1+close_massimo2)\n",
    "\n",
    "join_inc_perc = min_data_close.join(max_data_close)\n",
    "                                \n",
    "inc_perc = join_inc_perc.map(riga -> (ticker, (maxclose-minclose)/minclose * 100 round 2))\n",
    "\n",
    "result = inc_perc.join(sum_close_avg).join(sum_volume).sortBy(ticker)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Risultati\n",
    "Si riportano le prime 16 righe dell'output prodotto per il secondo job:\n",
    "\n",
    "```\n",
    "Sector\t\t\t Year  Total Volume  Percent Change\tDaily mean close price\n",
    "BASIC  INDUSTRIES  2004  30767395827   0.228150473063    2865.87369398\n",
    "BASIC  INDUSTRIES  2005  37457588379   0.0633341791619   4367.6822369\n",
    "BASIC  INDUSTRIES  2006  50413342778   0.294700074157    7110.07082055\n",
    "BASIC  INDUSTRIES  2007  67640775192   0.185667471534    9211.13392303\n",
    "BASIC  INDUSTRIES  2008  104336790359  -0.050131389643   7124.09400267\n",
    "BASIC  INDUSTRIES  2009  113161759706  0.0348287578952   4727.91920472\n",
    "BASIC  INDUSTRIES  2010  96267427694   0.217900355607    6126.05228338\n",
    "BASIC  INDUSTRIES  2011  93277620675   -0.586007636167   8535.55187455\n",
    "BASIC  INDUSTRIES  2012  79648935208   -0.687885248968   8694.81899382\n",
    "BASIC  INDUSTRIES  2013  81167036326   0.103226361955    28486.4249861\n",
    "BASIC  INDUSTRIES  2014  82010502666   -0.719021325857   24380.2074047\n",
    "BASIC  INDUSTRIES  2015  95592658398   -0.481011719633   9538.71135927\n",
    "BASIC  INDUSTRIES  2016  120096921114  0.138293577492    7721.85529589\n",
    "BASIC  INDUSTRIES  2017  100051884333  0.152790107615    9101.40840867\n",
    "BASIC  INDUSTRIES  2018  61239875499   -0.0307952148952  9883.13711755\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Grafici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Terzo job\n",
    "\n",
    "### 6.1 Specifiche\n",
    "Realizzare un job in grado di generare coppie di aziende di settori diversi le cui azioni, negli ultimi 3 anni, hanno avuto lo stesso trend in termini di variazione annuale indicando le aziende e il trend comune (es. Apple, Fiat, 2016:-1%, 2017:+3%, 2018:+5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Implementazione in MapReduce\n",
    "\n",
    "Per realizzare il questo job è stato necessario definire una applicazione MapReduce composta da due job. Vengono descritti in seguito a parole e tramite pseudocodice le funzionalità svolte da ciascun mapper e reducer dell'applicazione MapReduce realizzata.\n",
    "\n",
    "**Primo mapper**:\n",
    "\n",
    "Leggiamo dapprima, utilizzando la _Distributed Cache_, il file `historical_stocks.csv`, per poi definire, a partire dallo stesso file, una struttura dati `tickerToNameMap` che associa a ciascun `ticker` il nome della corrispondente compagnia.\n",
    "\n",
    "La struttura dati `tickerToCompanyNameMap` così creata verrà poi usata per poter effettuare in seguito il \"join\" con i record in input al mapper, associando a ciascun record il nome della compagnia associata al `ticker` presente nel record stesso.\n",
    "\n",
    "Successivamente, in fase di mapping, estrapoliamo da ciascun record i campi `ticker`, `close` e `date`, verifichiamo che il record sia relativo ad un anno che ricada nell'intervallo 2016-2018 per poi stampare la coppia chiave valore per il record in esame. \n",
    "\n",
    "In particolare la chiave è composta da tre campi: `name`, usata come chiave \"primaria\", `ticker` e `date`, dove questi ultimi due campi vengono usati per poter effettuare il _secondary sort_, in maniera tale che i valori per uno stesso nome di compagnia siano ordinati (e aggregati) per `name` e a parità di `name` siano ordinati per `ticker` e `data`.\n",
    "\n",
    "Grazie a questo ordinamento delle chiavi sarà poi possibile in fase di reduce poter calcolare gli incrementi percentuali associati a ciascun nome di azienda senza dover effettuare una comparazione tra date (al fine di trovare la data più recente e quella meno recente).\n",
    "\n",
    "Il valore associato alla chiave corrisponde al campo `close` del record.\n",
    "\n",
    "```python\n",
    "class mapper:\n",
    "    \n",
    "    setup():\n",
    "        read the historical_stocks.csv file from the Distributed Cache\n",
    "        tickerToCompanyNameMap = map which associate the respective company name for each ticker in historical_stocks.csv\n",
    "    \n",
    "    map(key, record):\n",
    "        ticker, _, close, _, _, _, _, date = data\n",
    "        year = getYear(date)\n",
    "        if year in range from 2016 to 2018 and \n",
    "           ticker has a corresponding name in tickerToNameMap:\n",
    "            companyName = ticker's corresponding name\n",
    "            key = name, ticker, date\n",
    "            value = close\n",
    "            Emit(key, value)\n",
    "```\n",
    "\n",
    "**Primo reducer**\n",
    "\n",
    "Leggiamo dapprima, utilizzando la _Distributed Cache_, il file `historical_stocks.csv`, per poi definire, a partire dallo stesso file, una struttura dati `companyNameToSectorMap` che associa a ciascuna compagnia il settore corrispondente (si ricorda come una compagnia può essere associata a più di un settore).\n",
    "\n",
    "La struttura dati `companyNameToSectorMap` così creata verrà poi usata per poter effettuare il \"join\" con le coppie chiave-valore in input al reducer, associando a ciascuna coppia chiave-valore in input il settore della compagnia in esame.\n",
    "\n",
    "Fatto ciò, in fase di reduce, per ciascun nome di compagnia, definiamo una serie di strutture dati (nello specifico mappe chiave valore) finalizzate al calcolo dell'incremento percentuale per settore per ciascun anno nell'intervallo 2016-2018\n",
    "\n",
    "In particolare, per calcolare la differenza percentuale si sfrutta l'ordinamento dei valori associati a ciascun nome di compagnia per poter estrarre i prezzi di chiusura meno recenti e più recenti di ciascun `ticker` in ciascun anno. Si sommano poi tali valori per ciascun anno per ottenere la \"quotazione di partenza\" annua e la \"quotazione finale\" annua per quella particolare compagnia. A partire da questi ultimi due valori si calcola infine la differenza percentuale.\n",
    "\n",
    "```python\n",
    "class reducer:\n",
    "    \n",
    "    setup():\n",
    "        read the historical_stocks.csv file from the Distributed Cache\n",
    "        companyNameToSectorMap = map which associate the respective sector(s) for each company name in historical_stocks.csv\n",
    "    \n",
    "    reduce(companyName, records):\n",
    "        sectorList = companyNameToSectorMap[companyName]\n",
    "        yearToStartingCloseValueMap = empty map\n",
    "        yearToEndingCloseValueMap = empty map\n",
    "        \n",
    "        for record in records:\n",
    "            day = get date field from the current record\n",
    "            year = get year value from the day variable\n",
    "            if the record's corresponding ticker has changed from the previous record:\n",
    "                yearToEndingCloseValueMap[previous year] += close field taken from the previous record\n",
    "                yearToStartingCloseValueMap[year] += close field taken from the current record\n",
    "            \n",
    "        percentChange = empty map\n",
    "  \n",
    "        for each year from 2016 to 2018:\n",
    "            difference = yearToEndingCloseValueMap[year] - yearToStartingCloseValueMap[year]\n",
    "            percentChange[year] = difference/yearToStartingCloseValueMap[year]\n",
    "        \n",
    "        for each sector in sectorList:\n",
    "            Emit(percentChange[2016], percentChange[2017], percentChange[2018], name, sector)\n",
    "```\n",
    "\n",
    "**Secondo mapper**\n",
    "\n",
    "Questo secondo mapper estrae da ciascun record i campi prodotti dal precedente reducer e produce in output una coppia chiave valore dove la chiave corrisponde alla tripla di differenze percentuali relative al 2016, 2017 e 2018, mentre il valore corrisponde al nome della compagnia e il settore corrispondente.\n",
    "\n",
    "In questo modo, in fase di reduce verranno aggregati sotto la stessa chiave (ovvero tripla di differenze percentuali) tutte le compagnie che hanno lo stesso \"trend\" in termini di differenza percentuale.\n",
    "\n",
    "```python\n",
    "class mapper:\n",
    "    \n",
    "    map(key, record):\n",
    "        percentChange2016, percentChange2017, percentChange2018, companyName, sector = record\n",
    "        key = percentChange2016, percentChange2017, percentChange2018\n",
    "        value = companyName, sector\n",
    "        Emit(key, value)\n",
    "```\n",
    "\n",
    "**Secondo reducer**\n",
    "\n",
    "In fase di reducer effettuaimo una scansione (in realtà due scansioni) dei record per poi stampare le coppie di aziende che hanno la stessa tripla e settori differenti.\n",
    "\n",
    "\n",
    "```python\n",
    "class reducer:\n",
    "\n",
    "    \n",
    "    reduce(percentTriplet, records):\n",
    "        companyPairList = list of company which have different sector and the same \"trend\" (in other words the same percent change triplet for 2016, 2017, 2018) \n",
    "        \n",
    "        for each companyPair in companyPairList:\n",
    "            emit(companyPair)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Implementazione in Hive\n",
    "\n",
    "Viene dapprima effettuato un join tra le tabelle che contengono i file `historical_stock_prices.csv` e `historical_stocks.csv` mettendo la tabella che contiene i record giornalieri a destra dell'operatore di join per migliorare l'efficienza dell'operazione. Durante l'operazione di join vengono filtrati i ticker che non sono associati ad alcun settore.\n",
    "Vengono in seguito definite 5 viste:\n",
    "\n",
    "- `name_data_min_max`, contenente la data più recente e meno recente per ogni ticker in ciascun anno;\n",
    "- `name_data_close_min` e `name_data_close_max`, che contengono il prezzo di chiusura associato alla data meno recente e quella più recente rispettivamente per ciascuna compagnia in ciascun anno;\n",
    "- `name_anno_close`, che contiene l'incremento percentuale per ciascuna compagnia in ciascun anno;\n",
    "- `name_anno_close_JOIN`, contenente coppie di compagnie di differenti settori che hanno la stessa differenza percentuale per un dato anno.\n",
    "\n",
    "Infine si effettua un self join di `name_anno_close_JOIN` per poter individuare coppie di aziende che appartengono a differenti settori e hanno gli stessi valori di differenza percentuale per gli anni 2016, 2017 e 2018.\n",
    "\n",
    "```SQL\n",
    "CREATE TABLE IF NOT EXISTS table_ex3 AS \n",
    "SELECT hs.name, hs.sector, hsp.data, hsp.close, hs.ticker\n",
    "FROM historical_stock AS hs JOIN historical_stock_prices AS hsp on hsp.ticker=hs.ticker\n",
    "WHERE YEAR(data)>=2016 AND YEAR(data)<=2018 AND hs.sector!='N/A';\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS name_data_min_max AS \n",
    "SELECT name, ticker, sector, YEAR(data) AS anno, min(TO_DATE(data)) AS min_data, max(TO_DATE(data)) AS max_data \n",
    "FROM table_ex3 \n",
    "GROUP BY name, ticker, sector, YEAR(data);\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS name_data_close_min AS \n",
    "SELECT b.name, a.sector, YEAR(b.min_data) AS anno, SUM(a.close) AS min_close \n",
    "FROM table_ex3 AS a, name_data_min_max AS b \n",
    "WHERE a.sector=b.sector AND a.data=b.min_data AND b.ticker=a.ticker\n",
    "GROUP BY b.name,a.sector, YEAR(b.min_data);\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS name_data_close_max AS \n",
    "SELECT b.name, a.sector, YEAR(b.max_data) AS anno, SUM(a.close) AS max_close \n",
    "FROM table_ex3 AS a, name_data_min_max AS b \n",
    "WHERE a.sector=b.sector AND a.data=b.max_data AND a.ticker=b.ticker\n",
    "GROUP BY b.name, a.sector, YEAR(b.max_data); \n",
    "\n",
    "CREATE TABLE IF NOT EXISTS name_anno_close AS \n",
    "SELECT mi.name, mi.sector, mi.anno, ROUND(((ma.max_close-mi.min_close)/mi.min_close *100 ) , 0) AS perc_var_anno\n",
    "FROM name_data_close_min AS mi, name_data_close_max AS ma\n",
    "WHERE mi.name=ma.name AND mi.anno=ma.anno AND mi.sector=ma.sector\n",
    "ORDER BY name, sector, anno;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS name_anno_close_JOIN AS\n",
    "SELECT n1.name AS name1, n2.name AS name2, n1.anno, n1.perc_var_anno\n",
    "FROM name_anno_close AS n1, name_anno_close AS n2\n",
    "WHERE n1.name!=n2.name AND n1.sector!=n2.sector AND n1.anno=n2.anno AND n1.perc_var_anno=n2.perc_var_anno;\n",
    "\n",
    "INSERT OVERWRITE LOCAL DIRECTORY 'output/'\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY '\\t'\n",
    "SELECT DISTINCT a.name1, a.name2, a.anno AS anno1, a.perc_var_anno AS perc_var_anno1, b.anno AS anno2, b.perc_var_anno AS perc_var_anno2, c.anno AS anno3, c.perc_var_anno AS perc_var_anno3\n",
    "FROM name_anno_close_JOIN AS a, name_anno_close_JOIN AS b, name_anno_close_JOIN AS c\n",
    "WHERE a.name1=b.name1 AND b.name1=c.name1 AND a.name2=b.name2 AND b.name2=c.name2 AND a.anno=2016 AND b.anno=2017 AND c.anno=2018\n",
    "ORDER BY name1, name2;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Implementazione in Spark\n",
    "\n",
    "Si descrive ora l'implementazione del job in Spark. Vengono principalmente definiti i seguenti `RDD`:\n",
    "\n",
    "- `min_data_close`, che contiene per ciascuna compagnia, in ciascun anno, la quotazione relativa alla data meno recente;\n",
    "- `max_data_close`, che contiene per ciascuna compagnia, in ciascun anno, la quotazione relativa alla data più recente;\n",
    "\n",
    "A partire dai due RDD definiti in precedenza viene definito un nuovo RDD chiamato `inc_perc` che ha come chiave la coppia name, sector e come valore l'incremento percentuale annuo.\n",
    "\n",
    "Successivamente viene definito un RDD chiamato `three_row` che contiene per ciascuna compagnia il relativo settore e gli incrementi percentuali relativi al 2016, al 2017 e al 2018.\n",
    "\n",
    "Infine, una volta filtrato da `three_row` le compagnie che hanno valori nulli di incrementi percentuali per anche solo uno degli anni nell'intervallo 2016-2018, viene effettuato un self join di `three_row` mantenendo solo le coppie di aziende che hanno settori differenti e stessi incrementi percentuali per gli anni che vanno dal 2016 al 2018\n",
    "\n",
    "```python\n",
    "hsp = RDD contenente il dataset historical_stock_prices.csv,\n",
    "      con i record relativi ai soli anni nell'intervallo 2016-2018\n",
    "hs = RDD contenente il dataset historical_stocks.csv,\n",
    "     filtrando i record in cui il campo sector assume valore \"N/A\"\n",
    "    \n",
    "join_hsp_hs = join tra gli RDD hs e hsp\n",
    "\n",
    "persist join_hsp_hs in memory (and optionally on disk in case the RDD does not fit in memory)\n",
    "\n",
    "min_data_close = join_hsp_hs.map(riga -> ((ticker, sector, name, anno), (close, data)))\n",
    "                            .reduceByKey(minimo((data1,close1), (data2,close2)))\n",
    "                            .map(riga -> ((settore, name, anno), close_minimo)) \n",
    "                            .reduceByKey(close_minimo1+close_minimo2)\n",
    "\n",
    "max_data_close = join_hsp_hs.map(riga -> ((ticker, sector, name, anno), (close, data)))\n",
    "                            .reduceByKey(massimo((data1,close1), (data2,close2)))\n",
    "                            .map(riga -> ((settore, name, anno), close_massimo)) \n",
    "                            .reduceByKey(close_massimo1+close_massimo2)\n",
    "\n",
    "join_inc_perc = min_data_close.join(max_data_close)\n",
    "\n",
    "inc_perc = join_inc_perc.map(riga -> (ticker, (maxclose-minclose)/minclose * 100 round 0)) \n",
    "\n",
    "three_row = inc_perc.map(riga -> ((name,sector), ([(anno,incremento percentuale)])))\n",
    "                    .reduceByKey(([(anno1,incremento percentuale1) concat (anno2,incremento percentuale2)]))\n",
    "                           \n",
    "three_row_clean = three_row.filter(riga -> filtro le righe che contengono valori non nulli di incremento percentuali negli anni 2016 2017 e 2018)\n",
    "\n",
    "three_row_clean_order = three_row_clean.map(riga -> ordino la lista [anno1,inc_perc1,anno2,inc_perc2,anno3,inc_perc3])\n",
    "                           \n",
    "result = three_row_clean_order.map(riga -> ([anno1,inc_perc1,anno2,inc_perc2,anno3,inc_perc3]),((name,sector)))\n",
    "\t\t .reduceByKey((name1,sector1)+(name2,sector2))\n",
    "         .filter(len([name,sector])>=2) # filtro le tuple per considerare solo le coppie di aziende\n",
    "         .filter(sector1!=sector2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Risultati\n",
    "\n",
    "Si mostra ora l'output prodotto per il terzo job\n",
    "\n",
    "```\n",
    "First company                 Second company                            2016        2017        2018\n",
    "STURM, RUGER & COMPANY, INC.  LINCOLN EDUCATIONAL SERVICES CORPORATION  2016: -14%  2017: 4%    2018: 8%\n",
    "LEAR CORPORATION              PENNYMAC FINANCIAL SERVICES, INC.         2016: 10%   2017: 33%   2018: -8%\n",
    "JMP GROUP LLC                 EBAY INC.                                 2016: 12%   2017: -10%  2018: -1%\n",
    "HERSHEY COMPANY (THE)         SPECTRUM BRANDS HOLDINGS, INC.            2016: 18%   2017: 9%    2018: -11%\n",
    "GENTEX CORPORATION            CALERES, INC.                             2016: 25%   2017: 3%    2018: 11%\n",
    "TERRENO REALTY CORPORATION    DUNKIN&#39; BRANDS GROUP, INC.            2016: 28%   2017: 23%   2018: 9%\n",
    "SIGNATURE BANK                COMPASS MINERALS INTERNATIONAL, INC.      2016: 3%    2017: -8%   2018: -15%\n",
    "COWEN INC.                    GROUP 1 AUTOMOTIVE, INC.                  2016: 4%    2017: -11%  2018: 8%\n",
    "MIDDLESEX WATER COMPANY       INTERNATIONAL BANCSHARES CORPORATION      2016: 64%   2017: -3%   2018: 18%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Grafici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
